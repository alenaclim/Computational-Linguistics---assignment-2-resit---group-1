{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distributional Semantic Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I implemented a word2vec technique, and to understand this I used the following resources:\n",
    "\n",
    "-the lectures for Dimensional Space and for Word2Vec, to understand the theory behind it;\n",
    "\n",
    "-these 3 tutorials about implementing word2vec in python:\n",
    "    https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "    https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/\n",
    "    https://github.com/buomsoo-kim/Word-embedding-with-Python/blob/master/word2vec/source%20code/word2vec.ipynb\n",
    "\n",
    "After going through them first, step by step, with this corpus, and urderstanding very good how to do it, \n",
    "I took your code apart to understand each section, and then managed to put the output of the word2vec model into a 2dnumpy.\n",
    "\n",
    "I didn't copy the code provided in these tutorials, but I used the technique explained by them, with the \n",
    "gensim library. I hope that's alright.\n",
    "\n",
    "Regarding the rho, I beat the baseline immediately after implementing word2vec instead of the cooccurrence \n",
    "technique (that you provided). But I tried to make it between .6 and .8, as you mentioned that would could as a \"good\"\n",
    "rho in general. By increasing the window size to 10, and the iterator to 15 or 20, I managed to do that on the dev set. \n",
    "Fingers crossed on the test set, no matter what I did, on this corpus I couldn't explain more than 1922 words :(\n",
    "I think I need to try other technique to get a higher rho than .627.\n",
    "\n",
    "Parameters: t=0 (frequecy), size = 100, window = 10, min-count = 1 (taking all words), iter = 20.\n",
    "\n",
    "Also, for the next part, I'll ask you where did you manage to download your corpus from. I tried so many websites, but \n",
    "I think even if I manage to download them, I cannot understand the files. But that's going to come next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement the word2vec method, I used the gensim library\n",
    "#pip install --upgrade gensim\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/_MYstuff/Desktop/Uni/Computational Linguistics/corpus.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = pd.read_csv('/_MYstuff/Desktop/Uni/Computational Linguistics/norms.dev.csv', header = 0, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this class I added one more method, that's calculating the word2vec model, and \n",
    "# I changed the self.embeddings to the 2dnumpy array I created instead of the cooccurences\n",
    "\n",
    "class SemanticSpace(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class creates distributional semantic space using a co-occurrence based approach.\n",
    "    \n",
    "    Any update you plan on doing (applying weighting schemes or dimensionality reduction, use a\n",
    "    prediction-based DSM, ...) need to be implemented in this class.\n",
    "    \n",
    "    In its current implementation, this class gets the corpus as a list of lists, with inner lists \n",
    "    consisting of strings. You can change anything you want: use different corpora, in different \n",
    "    formats, apply feature weighting schemes to the raw co-occurrences, apply dimensionality reduction,\n",
    "    learn the embeddings using prediction-based methods, ...\n",
    "    \n",
    "    The important things are:\n",
    "    - the SemanticSpace class needs to have the  word2idx attribute (a dictionary mapping strings to \n",
    "        integers, representing the row indices in the embedding space corresponding to each word). The \n",
    "        keys of this dictionary must be lower-cased tokens!\n",
    "    - the SemanticSpace class needs to have the embeddings attribute (a 2d NumPy array where rows are \n",
    "        words and columns are dimensions, symbolic or latent)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, t=10):\n",
    "        \n",
    "        \"\"\"\n",
    "        To initialize the semantic space it is enough to provide a corpus as a list of lists,\n",
    "        where each inner list consists of strings, representing the words in a sentence, and to\n",
    "        indicate a frequency threshold: only words with a frequency count higher or equal to the\n",
    "        threshold are used to build the semantic space.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        \n",
    "        # compute a word frequency distribution\n",
    "        self.freqs = self.freq_distr()\n",
    "        \n",
    "        # select words which occur more often than the threshold\n",
    "        self.targets = {w for w, f in self.freqs.items() if f > t}\n",
    "        \n",
    "        # map words to numerical indices\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.targets)}\n",
    "        \n",
    "        # update a co-occurrence matrix.\n",
    "        self.embeddings = self.word_2_vec()\n",
    "        \n",
    "        \"\"\"\n",
    "        IMPORTANT: the actual semantic space needs to be encoded as a NumPy 2d array!\n",
    "        Whatever transformation you decide to apply to raw counts (or whether you want to use Word2Vec)\n",
    "        make sure that self.embeddings is a 2d NumPy array with as many rows as there are words in the \n",
    "        vocabulary. I will compute cosine similarity indexing the embedding space!\n",
    "        \"\"\"\n",
    "    def word_2_vec(self):\n",
    "        # This far this was the best selection of the parameters\n",
    "        model = Word2Vec(self.corpus, size=100, sg = 1, window = 10, min_count = 1, iter = 20) \n",
    "        \n",
    "        rows = len(self.targets)\n",
    "        \n",
    "        vector_space = np.zeros([rows,100])\n",
    "        \n",
    "        for sentence in self.corpus:\n",
    "            for word in sentence:\n",
    "                if word in self.targets:\n",
    "                    vector_space[self.word2idx[word]] = model.wv[word]\n",
    "        return vector_space\n",
    "    \n",
    "    def freq_distr(self):\n",
    "        \n",
    "        word_frequencies = Counter()\n",
    "        for sentence in self.corpus:\n",
    "            for word in sentence:\n",
    "                word_frequencies[word] += 1\n",
    "        return word_frequencies\n",
    "    \n",
    "    def harvest_counts(self):\n",
    "        \n",
    "        # initialize an empty 2d NumPy array\n",
    "        cooccurrences = np.zeros((len(self.targets), len(self.targets)))\n",
    "        checkpoints = {int(len(self.corpus)*perc): perc*100 for perc in [0.2, 0.4, 0.6, 0.8, 1]}\n",
    "\n",
    "        for i, sentence in enumerate(self.corpus):\n",
    "            for word in sentence:\n",
    "                for context in sentence:\n",
    "                    # consider a whole sentence as context and update counts only between target words\n",
    "                    if word in self.targets and context in self.targets and word != context:\n",
    "                        cooccurrences[self.word2idx[word], self.word2idx[context]] += 1\n",
    "\n",
    "            # it's a long computation, let's make sure that we're making progress\n",
    "            if i+1 in checkpoints:\n",
    "                print(\"{}% of the corpus sentences have been processed at {}\".format(\n",
    "                    checkpoints[i+1], datetime.utcnow())\n",
    "                     )\n",
    "\n",
    "        return cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did not change anything in this class\n",
    "\n",
    "class Sim(object):\n",
    "    \n",
    "    \"\"\"This class compares semantic similarity scores retrieved from a corpus to human-generated norms.\"\"\"\n",
    "    \n",
    "    def __init__(self, norms, semantic_space):\n",
    "        \n",
    "        \"\"\"\n",
    "        This class is initialized providing two input structures:\n",
    "        - a Pandas DataFrame: the first column ('w1') contains the first word in the similarity pair, \n",
    "            the second column ('w2') contains the second word in the similarity pair, the third column \n",
    "            ('sim') contains the similarity score between w1 and w2.\n",
    "        - an object of class SemanticSpace (check the docs of this class for what it consists of and what \n",
    "            the necessary attributes it needs to have are)\n",
    "        \n",
    "        Don't change this class at all! Make sure that it works with the SemanticSpace class as you modify it.\n",
    "        I will use this class to evaluate your submissions, specifically the compute_correlation() method.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.norms = norms\n",
    "        self.embeddings = semantic_space.embeddings\n",
    "        self.word2idx = semantic_space.word2idx\n",
    "        \n",
    "    def compute_similarity(self, w1, w2):\n",
    "        \n",
    "        try:\n",
    "            e1 = self.embeddings[self.word2idx[w1], :]\n",
    "            try:\n",
    "                e2 = self.embeddings[self.word2idx[w2], :]\n",
    "                s = cosine_similarity(e1.reshape(1, -1), e2.reshape(1, -1))\n",
    "                return s[0][0]\n",
    "            \n",
    "            except KeyError:\n",
    "                print(\"Couldn't find the embedding for word {}. Not computing cosine for pair {}-{}.\".format(\n",
    "                    w2, w1, w2)\n",
    "                     )\n",
    "                \n",
    "        except KeyError:\n",
    "            print(\"Couldn't find the embedding for word {}. Not computing cosine for pair {}-{}.\".format(\n",
    "                    w1, w1, w2)\n",
    "                     )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def compute_correlation(self):\n",
    "        \n",
    "        true_similarities = []\n",
    "        estimated_similarities = []\n",
    "        for _, row in self.norms.iterrows():\n",
    "            s = self.compute_similarity(row['w1'], row['w2'])\n",
    "            if s:\n",
    "                estimated_similarities.append(s)\n",
    "                true_similarities.append(row['sim'])\n",
    "        \n",
    "        print(\"Pairs for which it was possible to compute cosine similarity: {}\".format(\n",
    "            len(estimated_similarities))\n",
    "             )\n",
    "        \n",
    "        print(\"Spearman rho between estimated and true similarity scores: {}\".format(\n",
    "            spearmanr(true_similarities, estimated_similarities)[0])\n",
    "             )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = SemanticSpace(data[0], t=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just checking that my idexes are correct (I had problems with the indexing in the beginning)\n",
    "\n",
    "#S.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Sim(norms, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find the embedding for word rodent. Not computing cosine for pair hummingbird-rodent.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair demolition-interior.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-paint.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair cathedral-interior.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair scenery-skyline.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-mushroom.\n",
      "Couldn't find the embedding for word lingerie. Not computing cosine for pair lingerie-sexy.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-room.\n",
      "Couldn't find the embedding for word canine. Not computing cosine for pair canine-husky.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-object.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-stair.\n",
      "Couldn't find the embedding for word gravestone. Not computing cosine for pair gravestone-silhouette.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair brick-skyline.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair concrete-interior.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair skyline-skyscraper.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair downtown-skyline.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair building-skyline.\n",
      "Couldn't find the embedding for word canine. Not computing cosine for pair canine-paw.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-restaurant.\n",
      "Couldn't find the embedding for word lingerie. Not computing cosine for pair kitten-lingerie.\n",
      "Couldn't find the embedding for word blur. Not computing cosine for pair blur-metro.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair ceiling-interior.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair dome-interior.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair bud-foliage.\n",
      "Couldn't find the embedding for word boardwalk. Not computing cosine for pair boardwalk-evening.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-frost.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-staircase.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-moss.\n",
      "Couldn't find the embedding for word gravestone. Not computing cosine for pair cemetery-gravestone.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-porch.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair bright-foliage.\n",
      "Couldn't find the embedding for word boardwalk. Not computing cosine for pair boardwalk-drop.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-orchid.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-tulip.\n",
      "Couldn't find the embedding for word rodent. Not computing cosine for pair insect-rodent.\n",
      "Couldn't find the embedding for word rodent. Not computing cosine for pair rodent-smile.\n",
      "Couldn't find the embedding for word gravestone. Not computing cosine for pair gravestone-pattern.\n",
      "Couldn't find the embedding for word canine. Not computing cosine for pair canine-run.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-roof.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair blossom-foliage.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair flower-foliage.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair demolition-skyline.\n",
      "Couldn't find the embedding for word gravestone. Not computing cosine for pair chapel-gravestone.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair daffodil-foliage.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair decoration-interior.\n",
      "Couldn't find the embedding for word rodent. Not computing cosine for pair lego-rodent.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair city-skyline.\n",
      "Couldn't find the embedding for word lingerie. Not computing cosine for pair lingerie-underground.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-fruit.\n",
      "Couldn't find the embedding for word mannequin. Not computing cosine for pair lock-mannequin.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair brick-interior.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair architecture-skyline.\n",
      "Couldn't find the embedding for word canine. Not computing cosine for pair canine-poodle.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair church-interior.\n",
      "Couldn't find the embedding for word blur. Not computing cosine for pair blur-lens.\n",
      "Couldn't find the embedding for word canine. Not computing cosine for pair canine-pug.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-tree.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-tower.\n",
      "Couldn't find the embedding for word gravestone. Not computing cosine for pair grave-gravestone.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-toy.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair horse-interior.\n",
      "Couldn't find the embedding for word rodent. Not computing cosine for pair mammal-rodent.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-skyscraper.\n",
      "Couldn't find the embedding for word blur. Not computing cosine for pair blur-eye.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-garden.\n",
      "Couldn't find the embedding for word gravestone. Not computing cosine for pair cop-gravestone.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-purple.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-tall.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-ivy.\n",
      "Couldn't find the embedding for word gravestone. Not computing cosine for pair gravestone-memorial.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-pink.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair flower-interior.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair bloom-foliage.\n",
      "Couldn't find the embedding for word skyline. Not computing cosine for pair skyline-tower.\n",
      "Couldn't find the embedding for word foliage. Not computing cosine for pair foliage-violet.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-skyline.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-outdoor.\n",
      "Couldn't find the embedding for word interior. Not computing cosine for pair interior-patio.\n",
      "Pairs for which it was possible to compute cosine similarity: 1922\n",
      "Spearman rho between estimated and true similarity scores: 0.6273466998188015\n"
     ]
    }
   ],
   "source": [
    "sim.compute_correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alena's Distributional Semantic Space (using word 2 vec): changing differenct parameters, and deciding on the best\n",
    "\n",
    "# t=0, size = 100, the rest were default, it explained 1922, rho=0.46\n",
    "# t=0, size = 100, window = 3, sg = 1, window = 3, min_count = 1, iter = 10, explained 1922, rho=0.56\n",
    "# t=0, size = 100, window = 5, ..., explained 1922, rho=0.595\n",
    "# t=10, size = 100, ..., explained 1660, rho=0.628 # not changing the frequency again haha\n",
    "# t=0, size = 100, window = 7, ..., explained 1992, rho = 0.602 #0.5973027394084918\n",
    "# t=0, size = 100, window = 10, ..., explained 1992, rho = 0.6145 # 0.6110385663024789 #0.617686246938297\n",
    "# size=150, window =10, 1922, 0.605426884330648\n",
    "# size-200, window = 10, 1922, 0.6030114262194551\n",
    "# size=100, window =10, sg=1, iter to 15 => 0.6245213133151273\n",
    "\n",
    "# size=100, window =10, sg=1, iter to 20 => 0.6273466998188015 -> THIS IS IT :D\n",
    "\n",
    "# size=300, window =10, sg=1, inter=20 => 1992 explained, rho = 0.6120727399179045 => i'll keep the previous score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giovanni's Distributional Semantic Space (using a cooccurrence technique)\n",
    "\n",
    "# t=0: 965 pairs in the test, 49K tokens in the embedding space, rho=0.2239\n",
    "# t=10: 829 pairs in the test, 12K tokens in the embedding space\n",
    "# t=25: 714 pairs in the test, 8K tokens in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
